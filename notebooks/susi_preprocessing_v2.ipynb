{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üö® This will only be needed for the training because for the predicting the image will be loaded/stored differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading as lists and then converting to arrays (Susi's approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì @Susi :\n",
    "- I think we need to \"generalized\" the whole data path => I defined it as a constant (will be in params)\n",
    "- I think that the CNN input are tensors... if we are using this kind of model, we should built a tensor (number of images, height, width, channels)\n",
    "- Using skimage library function to load images allows us to directly resize them => interesting to resize from the loading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_with_labels(root_folder):\n",
    "    images_with_labels = []\n",
    "    for object_folder in os.listdir(root_folder):\n",
    "        object_path = os.path.join(root_folder, object_folder)\n",
    "        if os.path.isdir(object_path):\n",
    "            for filename in os.listdir(object_path):\n",
    "                img_path = os.path.join(object_path, filename)\n",
    "                if os.path.isfile(img_path):\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        label = object_folder\n",
    "                        images_with_labels.append((img, label))\n",
    "    return images_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root folder containing subfolders for each object category\n",
    "    # current_directory = os.getcwd()\n",
    "    # root_folder = os.path.dirname(current_directory) + '/raw_data/Garbage classification'\n",
    "LOCAL_DATA_PATH = os.path.join(os.path.expanduser('~'), \".lewagon\", \"waste_sorter_smart_bin\", \"data\")\n",
    "root_folder = os.path.join(LOCAL_DATA_PATH,'raw','Garbage classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Ainhoa/.lewagon/waste_sorter_smart_bin/data/raw/Garbage classification'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper', '.DS_Store', 'metal', 'cardboard', 'trash', 'glass', 'plastic']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images with labels\n",
    "images_with_labels = load_images_with_labels(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, tuple, numpy.ndarray, str)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images_with_labels) , type(images_with_labels[0]), type(images_with_labels[0][0]), type(images_with_labels[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2527, 2, (384, 512, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_with_labels) , len(images_with_labels[0]), images_with_labels[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading directly as tensors from directory (Ainhoa's approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì @Susi : As the Kaggle dataset images are classified in folders named by its class, maybe is more convinient to load them as Tensorflow Datasets as we saw in the CNN challenges so that it is easier to handle them when training (making batches, etc). Also this might allow as to preprocess the images as tensors (more efficient) with tensorflow.keras functions like resizing.\n",
    "\n",
    "BUT I didn't manage to make it work so, for the moment, I followed your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory from where the notebook is run\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory where the Garbage classification images folder is stored\n",
    "data_dir = '../raw_data/Garbage classification'\n",
    "\n",
    "# check by listing all the elements in the directory\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as Tensorflow Dataset\n",
    "batch_size = None\n",
    "\n",
    "data = image_dataset_from_directory(\n",
    "  root_folder, # Folder directory\n",
    "  labels = \"inferred\", # inferred from sub folder name\n",
    "  label_mode = \"categorical\",\n",
    "  seed=None,\n",
    "  image_size=(224, 224), # we can resize directly the images\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# With Tensorflow Dataset object we can also define directly the training/validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class names\n",
    "data.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in data:\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üö® The preprocessing is going to depend on the architecture of the model (some include already the preprocessing) and, if we use transfer learning, we might have to use the specific preprocessing used in the pretrained model.\n",
    "This is the basic preprocessing for images and labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Susi's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì @Susi :\n",
    "- I don't like a lot to do the preprocessing image by image with lists (I think it would be faster with arrays/tensors) but I haven't find a way to resize tensors yet so I leave it like this for the moment.\n",
    "- I wasn't sure how to deal with 'string numpy.arrays' for the labels that you created. So I changed a bit the code and included the conversion from string-labels to numerical-categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img, target_size):\n",
    "    # Resize image\n",
    "    img_resized = cv2.resize(img, target_size)\n",
    "\n",
    "    # Normalize pixel values\n",
    "    img_normalized = img_resized / 255.0\n",
    "\n",
    "    return img_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define target size for resizing\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Preprocess each image in the dataset\n",
    "preprocessed_data = []\n",
    "for img, label in images_with_labels:\n",
    "    preprocessed_img = preprocess_image(img, target_size)\n",
    "    preprocessed_data.append((preprocessed_img, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert preprocessed_data to NumPy arrays\n",
    "X = np.array([data[0] for data in preprocessed_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([data[1] for data in preprocessed_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ainhoa's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "\n",
    "# Basic preprocess : Resize and normalize images and convert categories to numbers\n",
    "preprocessed_images = []\n",
    "labels_list = []\n",
    "for img, label in images_with_labels:\n",
    "    labels_list.append(label)\n",
    "    preprocessed_img = preprocess_image(img, target_size)\n",
    "    preprocessed_images.append(preprocessed_img)\n",
    "\n",
    "# Create a Pandas Series\n",
    "labels_series = pd.Series(labels_list)\n",
    "\n",
    "# Define a dictionary mapping each fruit to its length\n",
    "categories_map = {'glass': 1, 'paper': 2, 'cardboard': 3, 'plastic': 4, 'metal': 5, 'trash': 0}\n",
    "\n",
    "# Use the map() method to apply the mapping to each element of the Series\n",
    "categories_series = labels_series.map(categories_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to arrays\n",
    "images_array = np.array(preprocessed_images)\n",
    "labels_array = np.array(categories_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target encoding\n",
    "to_categorical(labels_array, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## VGG16 preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing VGG16 preprocessing input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target size for resizing\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Define root folder containing subfolders for each object category\n",
    "LOCAL_DATA_PATH = os.path.join(os.path.expanduser('~'), \".lewagon\", \"waste_sorter_smart_bin\", \"data\")\n",
    "root_folder = os.path.join(LOCAL_DATA_PATH,'raw','Garbage classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_MAP = {\n",
    "    'glass': 1,\n",
    "    'paper': 2,\n",
    "    'cardboard': 3,\n",
    "    'plastic': 4,\n",
    "    'metal': 5,\n",
    "    'trash': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_with_labels(root_folder, target_size=(244,244)):\n",
    "    ''' \n",
    "    Load images from local data folder\n",
    "    Convert them to arrays\n",
    "    Create a list with all the images and the equivalent with all the labels\n",
    "    '''\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "    for object_folder in os.listdir(root_folder):\n",
    "        object_path = os.path.join(root_folder, object_folder)\n",
    "        if os.path.isdir(object_path):\n",
    "            for filename in os.listdir(object_path):\n",
    "                img_path = os.path.join(object_path, filename)\n",
    "                if os.path.isfile(img_path):\n",
    "                    img = tf.keras.utils.load_img(img_path, target_size=target_size)\n",
    "                    img_array = tf.keras.utils.img_to_array(img)\n",
    "                    if img_array is not None:\n",
    "                        label = object_folder\n",
    "                        labels_list.append(label)\n",
    "                        images_list.append(img_array)\n",
    "    return images_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels_list):\n",
    "    ''' Converts the names of the labels in integer (category classes)'''\n",
    "    labels_series = pd.Series(labels_list)\n",
    "    categories_series = labels_series.map(CATEGORIES_MAP)\n",
    "    return categories_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_with_labels = load_images_with_labels(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 244, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_with_labels[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = images_with_labels[1]\n",
    "categories_series = preprocess_labels(labels_list)\n",
    "images_list = images_with_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to arrays\n",
    "images_array = np.array(images_list)\n",
    "labels_array = np.array(categories_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2527, 224, 224, 3), (2527,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_array.shape , labels_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = tf.keras.applications.vgg16.preprocess_input(images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        [ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        [ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        ...,\n",
       "        [ 8.5060997e+01,  8.7221001e+01,  8.9320000e+01],\n",
       "        [ 8.4060997e+01,  8.6221001e+01,  8.8320000e+01],\n",
       "        [ 8.3060997e+01,  8.5221001e+01,  8.7320000e+01]],\n",
       "\n",
       "       [[ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        [ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        [ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        ...,\n",
       "        [ 8.4060997e+01,  8.6221001e+01,  8.8320000e+01],\n",
       "        [ 8.3060997e+01,  8.5221001e+01,  8.7320000e+01],\n",
       "        [ 8.3060997e+01,  8.5221001e+01,  8.7320000e+01]],\n",
       "\n",
       "       [[ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        [ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        [ 1.1606100e+02,  1.1522100e+02,  1.1232000e+02],\n",
       "        ...,\n",
       "        [ 8.3060997e+01,  8.5221001e+01,  8.7320000e+01],\n",
       "        [ 8.3060997e+01,  8.5221001e+01,  8.7320000e+01],\n",
       "        [ 8.2060997e+01,  8.4221001e+01,  8.6320000e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.2060997e+01, -5.7789993e+00, -8.6800003e+00],\n",
       "        [ 5.9060997e+01,  1.1221001e+01,  8.3199997e+00],\n",
       "        [ 5.9060997e+01,  1.1221001e+01,  8.3199997e+00],\n",
       "        ...,\n",
       "        [ 2.0609970e+00, -4.9778999e+01, -5.4680000e+01],\n",
       "        [-1.9390030e+00, -5.3778999e+01, -5.8680000e+01],\n",
       "        [-1.9390030e+00, -5.1778999e+01, -5.6680000e+01]],\n",
       "\n",
       "       [[ 5.6060997e+01,  8.2210007e+00,  5.3199997e+00],\n",
       "        [ 4.9060997e+01,  1.2210007e+00, -1.6800003e+00],\n",
       "        [ 5.4060997e+01,  6.2210007e+00,  3.3199997e+00],\n",
       "        ...,\n",
       "        [ 1.0609970e+00, -5.1778999e+01, -5.6680000e+01],\n",
       "        [-2.9390030e+00, -5.4778999e+01, -5.9680000e+01],\n",
       "        [ 4.0609970e+00, -4.7778999e+01, -5.2680000e+01]],\n",
       "\n",
       "       [[ 6.3060997e+01,  1.5221001e+01,  1.2320000e+01],\n",
       "        [ 4.9060997e+01,  1.2210007e+00, -1.6800003e+00],\n",
       "        [ 6.2060997e+01,  1.4221001e+01,  1.1320000e+01],\n",
       "        ...,\n",
       "        [ 3.0609970e+00, -4.9778999e+01, -5.4680000e+01],\n",
       "        [-5.9390030e+00, -5.7778999e+01, -6.2680000e+01],\n",
       "        [ 6.0997009e-02, -5.1778999e+01, -5.6680000e+01]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final code for the .py files in \"logic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üö® Here I sum up the final code I put in the .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def load_images_with_labels(root_folder, target_size=(244,244)):\n",
    "    ''' \n",
    "    Load images from local data folder\n",
    "    Convert them to arrays\n",
    "    Create a list with all the images and the equivalent with all the labels\n",
    "    '''\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "    for object_folder in os.listdir(root_folder):\n",
    "        object_path = os.path.join(root_folder, object_folder)\n",
    "        if os.path.isdir(object_path):\n",
    "            for filename in os.listdir(object_path):\n",
    "                img_path = os.path.join(object_path, filename)\n",
    "                if os.path.isfile(img_path):\n",
    "                    img = tf.keras.utils.load_img(img_path, target_size=target_size)\n",
    "                    img_array = tf.keras.utils.img_to_array(img)\n",
    "                    if img_array is not None:\n",
    "                        label = object_folder\n",
    "                        labels_list.append(label)\n",
    "                        images_list.append(img_array)\n",
    "    return images_list, labels_list\n",
    "\n",
    "def preprocess_labels(labels_list):\n",
    "    ''' Converts the names of the labels in integer (category classes)'''\n",
    "    labels_series = pd.Series(labels_list)\n",
    "    categories_series = labels_series.map(CATEGORIES_MAP)\n",
    "    return categories_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root folder containing subfolders for each object category\n",
    "LOCAL_DATA_PATH = os.path.join(os.path.expanduser('~'), \".lewagon\", \"waste_sorter_smart_bin\", \"data\")\n",
    "root_folder = os.path.join(LOCAL_DATA_PATH,'raw','Garbage classification')\n",
    "\n",
    "# Load images with labels\n",
    "images_with_labels = load_images_with_labels(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists\n",
    "labels_list = images_with_labels[1]\n",
    "images_list = images_with_labels[0]\n",
    "\n",
    "# Transform categories to numbers\n",
    "categories_series = preprocess_labels(labels_list)\n",
    "\n",
    "# Convert to arrays\n",
    "images_array = np.array(preprocessed_images)\n",
    "labels_array = np.array(categories_series)\n",
    "\n",
    "# Target encoding\n",
    "to_categorical(labels_array, num_classes=6)\n",
    "\n",
    "# VGG16 preprocessing\n",
    "preprocessed_images = tf.keras.applications.vgg16.preprocess_input(images_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üö® I think we should either decide in a first base model and give it to the person that will do the modeling part, either talk with the team and decide all together which kind of model we will use (at least, the first layer type, so that we can adapt the preprocessing to it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(set(y))\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
